{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "fBOIaxCvcxRJ",
        "outputId": "285164d1-9fd8-4829-b85f-4425da5fcae0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.7-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docx\n",
            "  Downloading docx-0.2.4.tar.gz (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.9/54.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting pdfminer.six==20250506 (from pdfplumber)\n",
            "  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.2.1)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (3.4.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from docx) (5.4.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.65)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.45)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.6.15)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.14.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (24.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.22)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
            "Downloading pdfplumber-0.11.7-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: docx\n",
            "  Building wheel for docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx: filename=docx-0.2.4-py3-none-any.whl size=53893 sha256=5369de7e8112c73d8b29b1271c7905807d5e0fb0123044fa16536397772f87b3\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/3e/c3/e81c11effd0be5658a035947c66792dd993bcff317eae0e1ed\n",
            "Successfully built docx\n",
            "Installing collected packages: python-docx, pypdfium2, docx, pdfminer.six, pdfplumber\n",
            "Successfully installed docx-0.2.4 pdfminer.six-20250506 pdfplumber-0.11.7 pypdfium2-4.30.1 python-docx-1.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pdfplumber docx langchain requests python-docx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yh1VJEbAc6dr"
      },
      "outputs": [],
      "source": [
        "# Detects file type and extracts text from PDF, DOCX, or TXT (with pdfplumber for tables)\n",
        "import os\n",
        "import pdfplumber\n",
        "from docx import Document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rhC79Thjc6gZ"
      },
      "outputs": [],
      "source": [
        "def extract_text_from_file(file_path):\n",
        "    ext = os.path.splitext(file_path)[1].lower()\n",
        "\n",
        "    if ext == '.pdf':\n",
        "        text = \"\"\n",
        "        with pdfplumber.open(file_path) as pdf:\n",
        "            for page in pdf.pages:\n",
        "                text += page.extract_text() + \"\\n\"\n",
        "    elif ext == '.docx':\n",
        "        doc = Document(file_path)\n",
        "        text = \"\\n\".join([para.text for para in doc.paragraphs])\n",
        "    elif ext == '.txt':\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            text = f.read()\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file type\")\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dGX1LnHec6jr"
      },
      "outputs": [],
      "source": [
        "text=extract_text_from_file('/content/22117028_Default Resume_2025-06-22_18_49_06.pdf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6_H5kVcc6mv",
        "outputId": "a027c8c9-fa42-41be-d182-a493c71e65bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Cleans and normalizes extracted text using regex + NLP techniques\n",
        "import re\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "47AQ23bhc6pe"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def preprocess_text(text: str) -> str:\n",
        "    # Regex cleaning\n",
        "    text = re.sub(r'\\n{2,}', '\\n', text)\n",
        "    text = re.sub(r'[ \\t]+', ' ', text)\n",
        "    text = re.sub(r'\\n\\s*\\n', '\\n', text)\n",
        "    text = text.replace('\\xa0', ' ')\n",
        "    text = text.encode('ascii', 'ignore').decode()\n",
        "\n",
        "    # Remove repeating headers/footers\n",
        "    lines = text.splitlines()\n",
        "    line_counts = {}\n",
        "    for line in lines:\n",
        "        line_counts[line] = line_counts.get(line, 0) + 1\n",
        "    lines = [line for line in lines if line_counts[line] < 5]\n",
        "    text = \" \".join(lines)\n",
        "\n",
        "    # NLP preprocessing: stopword removal + lemmatization\n",
        "    doc = nlp(text)\n",
        "    cleaned = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct and token.is_alpha]\n",
        "    return \" \".join(cleaned)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "TeTBf5KBc6sy"
      },
      "outputs": [],
      "source": [
        "final_text=preprocess_text(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "x3iK1uimc6vs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "T60PmgK-c6zJ"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "jEn_IdP-dNHg"
      },
      "outputs": [],
      "source": [
        "def split_text_into_chunks(text, chunk_size=1200, chunk_overlap=50):\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap\n",
        "    )\n",
        "    return splitter.split_text(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8Q_eCqXpdNKT",
        "outputId": "2c66bce8-fa77-43f2-a9d8-fae169db89ab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Arpit Kumar Indian Institute UG III Year II Semester Technology Mechanical Engineering Contact Roorkee email registration VERIFIED Education Year Degree Examination Institution Board Percentage Year Indian Institute Technology Roorkee Intermediate Class XII sunshine sr sec school katrathal Matriculate Class X sunshine sr sec school katrathal Internships summer internship IIT Roorkee June July Development hybrid finishing process situ laser polish machine improve machinability laser additive manufacture component Business Analyst Mount Diggers October December Business Analyst Intern Conducted market research competitor analysis identify trend trekking industry drive strategic decision making analyzed customer datum improve service offering result enhance user satisfaction retention deliver actionable insight data visualization report support growth initiative Data Science CodTech solution January February gain experience field datum science machine learning Optimization Projects Hydraulic House Lifting System MIED Department IIT Roorkee August November design develop cad model lift house hydraulic system gather datum information static analysis system work minimize cause failure',\n",
              " 'analysis system work minimize cause failure system Potato disease classifier Jawahar Bhawan April clean preprocess dataset scale feature encode categorical variable train evaluate machine learning model dataset integrate train model Flask app real time prediction fast fashion clothing brand inspire Zudio MIED Department IIT Roorkee July November project aim establish dynamic trend drive fast fashion brand deliver affordable high quality apparel modern style conscious consumer inspire Zudios business model brand focus offer wide range contemporary design category include casual wear ethnic wear accessory footwear man woman child Predictive Analysis forecasting car sale Multivariate Data SAE IIT Roorkee August September develop predictive model forecast car sale base car property model price engine type fuel efficiency applied multivariate datum analysis machine learn technique identify key factor influence sale trend enable accurate demand forecasting well inventory planning Equipment Failure Prediction machine Learning SAE IIT Roorkee November January create predictive model machine learn technique forecast equipment failure base sensor datum relevant parameter select appropriate',\n",
              " 'datum relevant parameter select appropriate machine learning algorithm tailor dataset prediction requirement design predictive model handle large dataset efficiently real time batch processing Awards Scholarships Academic Achievements secure India Rank lakh student JEE Advanced Silver Medal intra bhawan football tournament Hod Appreciation Awardee excellent academic performance Positions Responsibility Extra Curriculars Member Bhawan Council Jawahar Bhawan IIT Roorkee March Present Worked hostel facility concern improve coordinate event Bhawan Day Bhawan Sports Tournament Executive Member IIT ROORKEE July June student organization IIT Roorkee pioneer trekking mountaineering adventure sport manage activity benefit mountain rural population associate Coordinator Student Technical Council IIT Roorkee October Present coordinate technical initiative ensure smooth execution event workshop multiple club contribute strategic planning decision making technical project initiative']"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "split_text_into_chunks(final_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "XIa5DINRdNMw"
      },
      "outputs": [],
      "source": [
        "PROMPT_TEMPLATE = \"\"\"\n",
        "You are a document analysis assistant designed to extract structured metadata and generate summaries from text.\n",
        "\n",
        "Your task is to read the content below and return the following information in a strict JSON format:\n",
        "1. **title**: The title of the document (or an inferred title based on its content).\n",
        "2. **author**: The author’s name, if available, or return \"Unknown\".\n",
        "3. Look for phrases like \"Date\", \"Published on\", \"Created on\", or any date-like string (e.g., YYYY-MM-DD or Month DD, YYYY).\n",
        "4. **keywords**: A list of 4–7 relevant keywords or key phrases based on the document's topics.\n",
        "5. **document_type**: Choose the most appropriate type from:\n",
        "   - \"research paper\", \"legal notice\", \"resume\", \"report\", \"book chapter\",\n",
        "     \"article\", \"business proposal\", \"letter\", or \"others\".\n",
        "6. **summary**: A concise, neutral summary (3–5 sentences) covering the main points.\n",
        "\n",
        " Return your response strictly in this JSON format (no explanation, no markdown):\n",
        "\n",
        "{{\n",
        "  \"title\": \"\",\n",
        "  \"author\": \"\",\n",
        "  \"date\": \"\",\n",
        "  \"keywords\": [],\n",
        "  \"document_type\": \"\",\n",
        "  \"summary\": \"\"\n",
        "}}\n",
        "\n",
        "Content to analyze:\n",
        "\\\"\\\"\\\"{content_chunk}\\\"\\\"\\\"\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "zwNmnz6ddNPm"
      },
      "outputs": [],
      "source": [
        "# 5. llm_call.py\n",
        "import os\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "wjFYCNfrdUcv"
      },
      "outputs": [],
      "source": [
        "os.environ[\"MISTRAL_API_URL\"] = \"https://api.mistral.ai/v1/chat/completions\"\n",
        "os.environ[\"MISTRAL_API_KEY\"] = \"xLy5WZAJHVd0AkGgkAcOO6X1psZWo0jY \"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Ih7RscmFdUfU"
      },
      "outputs": [],
      "source": [
        "MISTRAL_API_URL = os.getenv(\"MISTRAL_API_URL\")\n",
        "MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\")\n",
        "\n",
        "def call_llm_on_chunk(chunk):\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {os.getenv('MISTRAL_API_KEY')}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    data = {\n",
        "        \"model\": \"open-mistral-7b\",\n",
        "        \"messages\": [\n",
        "            {\"role\": \"user\", \"content\": PROMPT_TEMPLATE.format(content_chunk=chunk)}\n",
        "        ],\n",
        "        \"temperature\": 0.3\n",
        "    }\n",
        "    response = requests.post(os.getenv(\"MISTRAL_API_URL\"), headers=headers, json=data)\n",
        "    if response.status_code != 200:\n",
        "        print(f\"❌ Error {response.status_code}: {response.text}\")\n",
        "        return \"ERROR\"\n",
        "    return response.json()['choices'][0]['message']['content']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "adipayApdUiM"
      },
      "outputs": [],
      "source": [
        "def summarize_document_chunks(chunks):\n",
        "    results = []\n",
        "    for chunk in chunks:\n",
        "        result = call_llm_on_chunk(chunk)\n",
        "        results.append(result)\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "b3fiHRwCdUk_"
      },
      "outputs": [],
      "source": [
        "text = extract_text_from_file('/content/22117028_Default Resume_2025-06-22_18_49_06.pdf')\n",
        "clean_text = preprocess_text(text)\n",
        "chunks = split_text_into_chunks(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Bix_utAidUoe",
        "outputId": "f72e235c-c56d-4894-8244-f9a64ec4431d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keybert\n",
            "  Downloading keybert-0.9.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from keybert) (2.0.2)\n",
            "Requirement already satisfied: rich>=10.4.0 in /usr/local/lib/python3.11/dist-packages (from keybert) (13.9.4)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.11/dist-packages (from keybert) (1.6.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.52.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.33.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.4.0->keybert) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.4.0->keybert) (2.19.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.2->keybert) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.2->keybert) (3.6.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.4.0->keybert) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.6.15)\n",
            "Downloading keybert-0.9.0-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.4/41.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m109.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, keybert\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed keybert-0.9.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install keybert sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "Xy9qo3TAdNSc"
      },
      "outputs": [],
      "source": [
        "from keybert import KeyBERT\n",
        "from sentence_transformers import SentenceTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pz0zJwbydeVW",
        "outputId": "721fa5f7-eb96-4038-80b3-06f7db2a76b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Total Chunks: 3\n",
            "\n",
            "--- Generating summary for Chunk 1/3 ---\n",
            "{\n",
            "  \"title\": \"Arpit Kumar's Academic and Professional Profile\",\n",
            "  \"author\": \"Arpit Kumar\",\n",
            "  \"date\": \"20XX-XX-XX (Inferred from the mention of II Semester, Class XII, and Internships dates)\",\n",
            "  \"keywords\": [\"Mechanical Engineering\", \"Indian Institute of Technology Roorkee\", \"Internship\", \"Business Analyst\", \"Data Science\", \"Machine Learning\", \"Hydraulic House Lifting System\"],\n",
            "  \"document_type\": \"Resume\",\n",
            "  \"summary\": \"This document details Arpit Kumar's academic and professional background. He is a Mechanical Engineering student at the Indian Institute of Technology Roorkee, currently in his III Year II Semester. The document mentions his verified email registration and his education, including his Matriculate Class X and Intermediate Class XII from Sunshine Sr Sec School Katrathal. Arpit has also completed summer internships at IIT Roorkee, where he worked on a hybrid finishing process using a situ laser polish machine to improve machinability and participated in laser additive manufacture. Additionally, he has experience as a Business Analyst, conducting market research, competitor analysis, and trend analysis for the trekking industry to drive strategic decision making and improve service offerings. He also mentions his work on an Optimization Project for the Hydraulic House Lifting System at IIT Roorkee's MIED Department.\"\n",
            "}\n",
            "\n",
            "--- Generating summary for Chunk 2/3 ---\n",
            "{\n",
            "  \"title\": \"Potato Disease Classifier and Fast Fashion Trend Prediction: A Multidisciplinary Approach\",\n",
            "  \"author\": \"Unknown\",\n",
            "  \"date\": \"April (for Potato Disease Classifier) and November (for Fast Fashion Trend Prediction)\",\n",
            "  \"keywords\": [\"Potato Disease Classifier\", \"Fast Fashion\", \"Trend Prediction\", \"Machine Learning\", \"Data Preprocessing\", \"Feature Engineering\", \"Flask App\", \"Real Time Prediction\", \"Zudio MIED\", \"IIT Roorkee\"],\n",
            "  \"document_type\": \"Research Paper\",\n",
            "  \"summary\": \"This document discusses two projects: a Potato Disease Classifier and a Fast Fashion Trend Prediction system. The Potato Disease Classifier project involves the development of a machine learning model to identify diseases in potatoes. The Fast Fashion Trend Prediction project aims to establish a dynamic trend for fast fashion brands, delivering affordable high-quality apparel with modern styles that cater to conscious consumers. The Fast Fashion project also includes the development of a predictive model for a fast fashion brand like Zudio, focusing on offering a wide range of contemporary designs, including casual wear, ethnic wear, accessories, footwear for men, women, and children.\"\n",
            "}\n",
            "\n",
            "--- Generating summary for Chunk 3/3 ---\n",
            "{\n",
            "  \"title\": \"Achievements and Positions Held by a Student at IIT Roorkee\",\n",
            "  \"author\": \"Unknown\",\n",
            "  \"date\": \"March Present (for hostel work) | July June (for student organization work) | October Present (for Student Technical Council work)\",\n",
            "  \"keywords\": [\"IIT Roorkee\", \"Academic Achievements\", \"Scholarships\", \"Awards\", \"Extra Curriculars\", \"Student Organization\", \"Trekking\", \"Mountaineering\", \"Mountain Rural Population\"],\n",
            "  \"document_type\": \"others\",\n",
            "  \"summary\": \"This document details the achievements and positions held by a student at IIT Roorkee. The student has received the Silver Medal in the intra bhawan football tournament and has been appreciated for excellent academic performance. They have also worked to improve the hostel facility, coordinate events such as Bhawan Day and Bhawan Sports Tournament, and served as an Executive Member of IIT ROORKEE. Additionally, they are currently the Coordinator of the Student Technical Council and have been managing technical initiatives and ensuring smooth execution of events and workshops, contributing to multiple clubs and engaging in strategic planning and decision making.\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "print(f\"✅ Total Chunks: {len(chunks)}\")\n",
        "\n",
        "results = []\n",
        "for i, chunk in enumerate(chunks):\n",
        "    print(f\"\\n--- Generating summary for Chunk {i+1}/{len(chunks)} ---\")\n",
        "    summary = call_llm_on_chunk(chunk)\n",
        "    print(summary)\n",
        "    results.append(summary)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "3nDtIIg_deYQ"
      },
      "outputs": [],
      "source": [
        "combine_prompt = f\"\"\"\n",
        "You are a smart metadata assistant. Below are partial summaries of a document generated from different chunks.\n",
        "\n",
        "Your task is to carefully read the partial summaries and synthesize them into a **single, coherent metadata JSON object** with meaningful, deduplicated values.\n",
        "\n",
        "Follow these rules:\n",
        "\n",
        "1. **title**: Infer a clear, meaningful title based on the content. If no strong title is evident, generate a short descriptive one.\n",
        "2. **author**: If a name is mentioned, use it. Otherwise, return \"Not specified\".\n",
        "3. **date**: Look for any date patterns in the summaries (e.g., '2024-11-10', 'Nov 10, 2024'). If not found, return \"Not specified\".\n",
        "4. **keywords**: Merge, deduplicate, and refine keywords from all chunks. Return 4–7 concise keywords.\n",
        "5. **document_type**: Use \"Article\" unless there's a clear reason to choose another from: [\"research paper\", \"legal notice\", \"resume\", \"report\", \"book chapter\", \"article\", \"business proposal\", \"letter\", \"others\"].\n",
        "6. **summary**: Write a clean, neutral, 3–5 sentence summary that captures the overall meaning of the full document.\n",
        "\n",
        "Return the result in this strict JSON format (no markdown, no commentary):\n",
        "\n",
        "{{\n",
        "  \"title\": \"Meaningful title of the whole document\",\n",
        "  \"author\": \"Author name (or 'Not specified')\",\n",
        "  \"date\": \"Extracted date if available, otherwise 'Not specified'\",\n",
        "  \"keywords\": [\"keyword1\", \"keyword2\", \"...\"],\n",
        "  \"document_type\": \"Article\",\n",
        "  \"summary\": \"Clean, concise summary of the full document.\"\n",
        "}}\n",
        "\n",
        "Here are the partial summaries:\n",
        "\\\"\\\"\\\"{combined_summaries}\\\"\\\"\\\"\n",
        "\"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "UiaJIBxcdebL"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import textwrap\n",
        "import re\n",
        "from keybert import KeyBERT\n",
        "from sentence_transformers import SentenceTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "r2d8CkGGdeeL"
      },
      "outputs": [],
      "source": [
        "def call_llm_merge_summary(prompt):\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {os.getenv('MISTRAL_API_KEY')}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    data = {\n",
        "        \"model\": \"open-mistral-7b\",\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "        \"temperature\": 0.3\n",
        "    }\n",
        "    response = requests.post(os.getenv(\"MISTRAL_API_URL\"), headers=headers, json=data)\n",
        "    return response.json()['choices'][0]['message']['content']\n",
        "\n",
        "final_output = call_llm_merge_summary(combine_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pn80oBrgdesU",
        "outputId": "7ab629b5-c802-42ea-a415-eaa0adaf6171"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Final Metadata:\n",
            "{\n",
            "  \"title\": \"Predictive Analysis in Automotive Industry: Car Sales and Equipment Failure at SAE IIT Roorkee\",\n",
            "  \"author\": \"Not specified\",\n",
            "  \"date\": \"August-September and November-January\",\n",
            "  \"keywords\": [\n",
            "    \"design category\",\n",
            "    \"class xii\",\n",
            "    \"preprocess dataset\",\n",
            "    \"gain experience\",\n",
            "    \"data sae\",\n",
            "    \"model dataset\",\n",
            "    \"machine improve\",\n",
            "    \"internships summer\",\n",
            "    \"indian institute\",\n",
            "    \"katrathal internships\"\n",
            "  ],\n",
            "  \"document_type\": \"Research Paper\",\n",
            "  \"summary\": \"This document discusses the development of two predictive models: one for car sales and another for equipment failure, using multivariate data and machine learning techniques. The models aim to accurately forecast car sales and identify key factors influencing equipment failure, respectively, for efficient inventory planning and maintenance.\"\n",
            "}\n",
            "\n",
            "✅ Final Summary:\n",
            "This document discusses the development of two predictive models: one for car sales and another for\n",
            "equipment failure, using multivariate data and machine learning techniques. The models aim to\n",
            "accurately forecast car sales and identify key factors influencing equipment failure, respectively,\n",
            "for efficient inventory planning and maintenance.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import re\n",
        "import textwrap\n",
        "from keybert import KeyBERT\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Example input: Replace this with your actual final_output and clean_text\n",
        "\n",
        "\n",
        "# ============================\n",
        "# 🎯 Final Output Handling\n",
        "# ============================\n",
        "try:\n",
        "    # ① Try fenced ```json ``` block\n",
        "    json_match = re.search(r'```json\\s+(.*?)\\s+```', final_output, re.DOTALL)\n",
        "\n",
        "    # ② If no fenced block found, try any JSON object in text\n",
        "    if not json_match:\n",
        "        json_match = re.search(r'\\{.*\\}', final_output, re.DOTALL)\n",
        "\n",
        "    # ③ If JSON found, parse it\n",
        "    if json_match:\n",
        "        json_string = json_match.group(0)\n",
        "        parsed = json.loads(json_string)\n",
        "\n",
        "        # ✅ Improve keywords using KeyBERT\n",
        "        if 'clean_text' in locals():\n",
        "            kw_model = KeyBERT(model=SentenceTransformer('all-MiniLM-L6-v2'))\n",
        "            kb_keywords = kw_model.extract_keywords(\n",
        "                clean_text,\n",
        "                keyphrase_ngram_range=(1, 2),\n",
        "                stop_words='english',\n",
        "                top_n=10,\n",
        "                use_maxsum=True,\n",
        "                nr_candidates=20\n",
        "            )\n",
        "            final_keywords = []\n",
        "            seen = set()\n",
        "            for kw, _ in kb_keywords:\n",
        "                if kw not in seen:\n",
        "                    seen.add(kw)\n",
        "                    final_keywords.append(kw)\n",
        "            parsed[\"keywords\"] = final_keywords\n",
        "        else:\n",
        "            print(\"⚠️ Warning: 'clean_text' not available for keyword extraction using KeyBERT.\")\n",
        "\n",
        "        # ✅ Pretty output\n",
        "        print(\"\\n✅ Final Metadata:\")\n",
        "        print(json.dumps(parsed, indent=2))\n",
        "\n",
        "        print(\"\\n✅ Final Summary:\")\n",
        "        if \"summary\" in parsed and parsed[\"summary\"]:\n",
        "            print(textwrap.fill(parsed[\"summary\"], width=100))\n",
        "        else:\n",
        "            print(\"Summary not available in the parsed output.\")\n",
        "\n",
        "    else:\n",
        "        print(\"⚠️ Could not find the JSON object within the final output string.\")\n",
        "        print(\"Showing raw output:\")\n",
        "        print(final_output)\n",
        "\n",
        "except json.JSONDecodeError as e:\n",
        "    print(f\"⚠️ JSONDecodeError: {e}\")\n",
        "    print(\"Showing raw output:\")\n",
        "    print(final_output)\n",
        "except KeyError as e:\n",
        "    print(f\"⚠️ KeyError: {e} - Check if expected keys are present in the JSON output.\")\n",
        "    if 'parsed' in locals():\n",
        "        print(\"Parsed dictionary (partial):\")\n",
        "        print(json.dumps(parsed, indent=2))\n",
        "    else:\n",
        "        print(\"Parsed dictionary not available.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "hOeKqAVJgm_8"
      },
      "outputs": [],
      "source": [
        "#!pip install streamlit pyngrok --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "AzJnWYAMy0mw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56bbdc8c-41ce-4b1a-eabe-76c4bdc36389"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m78.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "! pip install streamlit -q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rV5BfLNKDqwp",
        "outputId": "46d09d6a-da20-4df9-8f46-a40fe908dc03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (24.2)\n",
            "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.13\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!pip install pytesseract pillow\n",
        "!apt-get install tesseract-ocr -y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2REES6yDy0pW",
        "outputId": "84893759-52e7-4297-8437-6beac084bba0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "# app.py\n",
        "import os\n",
        "import json\n",
        "import textwrap\n",
        "import tempfile\n",
        "from pathlib import Path\n",
        "from typing import List\n",
        "\n",
        "import streamlit as st\n",
        "import requests\n",
        "from keybert import KeyBERT\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from docx import Document\n",
        "import pdfplumber\n",
        "\n",
        "# Load KeyBERT model once\n",
        "kw_model = KeyBERT()\n",
        "\n",
        "# Prompt used per chunk\n",
        "PROMPT_TEMPLATE = \"\"\"\n",
        "You are an intelligent assistant. Read this content chunk and return:\n",
        "- A 1-2 sentence summary\n",
        "- Five keywords (comma-separated)\n",
        "\n",
        "Chunk:\n",
        "\\\"\\\"\\\"{content_chunk}\\\"\\\"\\\"\n",
        "\"\"\"\n",
        "\n",
        "# ───────────────────────────────\n",
        "# Utility Functions\n",
        "# ───────────────────────────────\n",
        "\n",
        "def extract_text_from_file(uploaded_file) -> str:\n",
        "    suffix = Path(uploaded_file.name).suffix.lower()\n",
        "    with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp:\n",
        "        tmp.write(uploaded_file.read())\n",
        "        tmp_path = Path(tmp.name)\n",
        "\n",
        "    if suffix == \".txt\":\n",
        "        return tmp_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "    elif suffix == \".docx\":\n",
        "        doc = Document(tmp_path)\n",
        "        return \"\\n\".join([p.text for p in doc.paragraphs])\n",
        "    elif suffix == \".pdf\":\n",
        "        text = \"\"\n",
        "        with pdfplumber.open(tmp_path) as pdf:\n",
        "            for page in pdf.pages:\n",
        "                text += page.extract_text() or \"\"\n",
        "        return text\n",
        "    raise ValueError(f\"Unsupported file type: {suffix}\")\n",
        "\n",
        "def preprocess_text(text: str) -> str:\n",
        "    return \" \".join(text.split())\n",
        "\n",
        "def split_text_into_chunks(text: str, size: int = 1700, overlap: int = 50) -> List[str]:\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=size, chunk_overlap=overlap)\n",
        "    return splitter.split_text(text)\n",
        "\n",
        "def call_mistral(prompt: str, temperature: float = 0.3) -> str:\n",
        "    api_url = os.getenv(\"MISTRAL_API_URL\")\n",
        "    api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
        "    if not api_url or not api_key:\n",
        "        raise ValueError(\"Please set MISTRAL_API_URL and MISTRAL_API_KEY env vars\")\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {api_key}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    data = {\n",
        "        \"model\": \"open-mistral-7b\",\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "        \"temperature\": temperature\n",
        "    }\n",
        "    resp = requests.post(api_url, headers=headers, json=data, timeout=60)\n",
        "    resp.raise_for_status()\n",
        "    return resp.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "\n",
        "# ───────────────────────────────\n",
        "# Streamlit UI\n",
        "# ───────────────────────────────\n",
        "\n",
        "st.set_page_config(page_title=\"Metadata & Summary Generator\", layout=\"centered\")\n",
        "\n",
        "st.markdown(\n",
        "    '<h1 style=\"color:#4A90E2;text-align:center;\">📄 Auto Metadata & Summary Generator</h1>',\n",
        "    unsafe_allow_html=True\n",
        ")\n",
        "\n",
        "file = st.file_uploader(\"📂 Upload PDF, DOCX, or TXT file\", type=[\"pdf\", \"docx\", \"txt\"])\n",
        "\n",
        "if file:\n",
        "    with st.spinner(\"⏳ Processing the uploaded file...\"):\n",
        "        raw_text = extract_text_from_file(file)\n",
        "        clean_text = preprocess_text(raw_text)\n",
        "        chunks = split_text_into_chunks(clean_text)\n",
        "\n",
        "        summaries = [call_mistral(PROMPT_TEMPLATE.format(content_chunk=chunk)) for chunk in chunks]\n",
        "        combined = \"\\n\\n\".join(summaries)\n",
        "        # Capture first 5 lines to extract potential metadata\n",
        "        header_block = \"\\n\".join(clean_text.splitlines()[:5])\n",
        "        combined_with_header = header_block + \"\\n\\n\" + combined\n",
        "        # Final summarization prompt\n",
        "        final_prompt = f\"\"\"\n",
        "You are a smart metadata assistant. Below are partial summaries of a document generated from different chunks.\n",
        "\n",
        "Your task is to carefully read the partial summaries and synthesize them into a **single, coherent metadata JSON object** with meaningful, deduplicated values.\n",
        "\n",
        "Follow these rules:\n",
        "\n",
        "1. **title**: Infer a clear, meaningful title based on the content. If no strong title is evident, generate a short descriptive one.\n",
        "2. **author**: If a name is mentioned, use it. Otherwise, return \"Not specified\".\n",
        "3. **date**: Look for any date patterns in the summaries (e.g., '2024-11-10', 'Nov 10, 2024'). If not found, return \"Not specified\".\n",
        "4. **keywords**: Merge, deduplicate, and refine keywords from all chunks. Return 4–7 concise keywords.\n",
        "5. **document_type**: Use \"Article\" unless there's a clear reason to choose another from: [\"research paper\", \"legal notice\", \"resume\", \"report\", \"book chapter\", \"article\", \"business proposal\", \"letter\", \"others\"].\n",
        "6. **summary**: Write a clean, neutral, 3–5 sentence summary that captures the overall meaning of the full document.\n",
        "\n",
        "Return the result in this strict JSON format (no markdown, no commentary):\n",
        "\n",
        "{{\n",
        "  \"title\": \"Meaningful title of the whole document\",\n",
        "  \"author\": \"Author name (or 'Not specified')\",\n",
        "  \"date\": \"Extracted date if available, otherwise 'Not specified'\",\n",
        "  \"keywords\": [\"keyword1\", \"keyword2\", \"...\"],\n",
        "  \"document_type\": \"Article\",\n",
        "  \"summary\": \"Clean, concise summary of the full document.\"\n",
        "}}\n",
        "\n",
        "Here are the partial summaries:\n",
        "\\\"\\\"\\\"{combined_with_header}\\\"\\\"\\\"\n",
        "\"\"\"\n",
        "        final_output = call_mistral(final_prompt)\n",
        "\n",
        "    try:\n",
        "        parsed = json.loads(final_output)\n",
        "\n",
        "        # Optional: Use KeyBERT for better keywords\n",
        "        kb_keywords = kw_model.extract_keywords(\n",
        "            clean_text,\n",
        "            keyphrase_ngram_range=(1, 2),\n",
        "            stop_words=\"english\",\n",
        "            top_n=10,\n",
        "            use_maxsum=True,\n",
        "            nr_candidates=20\n",
        "        )\n",
        "        parsed[\"keywords\"] = [kw for kw, _ in kb_keywords]\n",
        "\n",
        "        # ── Styled Output ──\n",
        "        st.markdown('<h3 style=\"color:#1f77b4;\">📌 <b>Extracted Metadata</b></h3>', unsafe_allow_html=True)\n",
        "        st.json(parsed)\n",
        "\n",
        "        st.markdown('<h3 style=\"color:#2ca02c;\">📝 <b>Wrapped Summary</b></h3>', unsafe_allow_html=True)\n",
        "        st.markdown(\n",
        "            f\"<div style='color:#333333; font-size:16px; line-height:1.6; background-color:#f4f4f4; padding:15px; border-radius:8px'>{parsed['summary']}</div>\",\n",
        "            unsafe_allow_html=True\n",
        "        )\n",
        "        st.markdown(\"<br>\", unsafe_allow_html=True)\n",
        "\n",
        "        st.download_button(\n",
        "            label=\"💾 Download Summary\",\n",
        "            data=parsed[\"summary\"],\n",
        "            file_name=\"summary.txt\",\n",
        "            mime=\"text/plain\"\n",
        "        )\n",
        "\n",
        "        st.markdown(\"<hr><div style='text-align:center;color:#888'>Made with ❤️ by Arpit · Powered by Mistral AI</div>\", unsafe_allow_html=True)\n",
        "\n",
        "    except Exception as e:\n",
        "        st.error(f\"❌ Failed to parse output: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "HZG6Lfbh5vDi",
        "outputId": "fd26f2b8-38d3-4ec8-db33-02d364b30f03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K⠙\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.60.200.4:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0Kyour url is: https://busy-mice-wave.loca.lt\n",
            "2025-06-25 03:29:04.585892: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1750822144.615751   15659 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1750822144.624135   15659 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "# In Colab or local terminal:\n",
        "!streamlit run app.py & npx localtunnel --port 8501\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "ypZfAqza5vF9"
      },
      "outputs": [],
      "source": [
        "with open(\"requirements.txt\", \"w\") as f:\n",
        "    f.write(\"\"\"streamlit\n",
        "keybert\n",
        "pdfplumber\n",
        "python-docx\n",
        "requests\n",
        "langchain\n",
        "scikit-learn\n",
        "sentence-transformers\n",
        "pytesseract\n",
        "pillow\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "vTyg8d7r5vIp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "HaDbDhc-5vMI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "zy-Lc40t3pRM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}