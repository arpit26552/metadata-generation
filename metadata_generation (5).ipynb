{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "id": "fBOIaxCvcxRJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "994b34a8-0415-49f5-a881-b5ad49741cca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.7-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docx\n",
            "  Downloading docx-0.2.4.tar.gz (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.9/54.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting pdfminer.six==20250506 (from pdfplumber)\n",
            "  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.2.1)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (3.4.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from docx) (5.4.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.65)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.45)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.6.15)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.14.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (24.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.22)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
            "Downloading pdfplumber-0.11.7-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: docx\n",
            "  Building wheel for docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx: filename=docx-0.2.4-py3-none-any.whl size=53893 sha256=bdad0b4d60d8cf77038256b6e2f9dbea5b23dce4737a4870f4d3dbde92874db9\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/3e/c3/e81c11effd0be5658a035947c66792dd993bcff317eae0e1ed\n",
            "Successfully built docx\n",
            "Installing collected packages: python-docx, pypdfium2, docx, pdfminer.six, pdfplumber\n",
            "Successfully installed docx-0.2.4 pdfminer.six-20250506 pdfplumber-0.11.7 pypdfium2-4.30.1 python-docx-1.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pdfplumber docx langchain requests python-docx"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Detects file type and extracts text from PDF, DOCX, or TXT (with pdfplumber for tables)\n",
        "import os\n",
        "import pdfplumber\n",
        "from docx import Document"
      ],
      "metadata": {
        "id": "yh1VJEbAc6dr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_file(file_path):\n",
        "    ext = os.path.splitext(file_path)[1].lower()\n",
        "\n",
        "    if ext == '.pdf':\n",
        "        text = \"\"\n",
        "        with pdfplumber.open(file_path) as pdf:\n",
        "            for page in pdf.pages:\n",
        "                text += page.extract_text() + \"\\n\"\n",
        "    elif ext == '.docx':\n",
        "        doc = Document(file_path)\n",
        "        text = \"\\n\".join([para.text for para in doc.paragraphs])\n",
        "    elif ext == '.txt':\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            text = f.read()\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file type\")\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "rhC79Thjc6gZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=extract_text_from_file('/content/generated_sample_3000_words.pdf')"
      ],
      "metadata": {
        "id": "dGX1LnHec6jr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleans and normalizes extracted text using regex + NLP techniques\n",
        "import re\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6_H5kVcc6mv",
        "outputId": "ff7f6da9-a032-4a83-9668-40565aee71d4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def preprocess_text(text: str) -> str:\n",
        "    # Regex cleaning\n",
        "    text = re.sub(r'\\n{2,}', '\\n', text)\n",
        "    text = re.sub(r'[ \\t]+', ' ', text)\n",
        "    text = re.sub(r'\\n\\s*\\n', '\\n', text)\n",
        "    text = text.replace('\\xa0', ' ')\n",
        "    text = text.encode('ascii', 'ignore').decode()\n",
        "\n",
        "    # Remove repeating headers/footers\n",
        "    lines = text.splitlines()\n",
        "    line_counts = {}\n",
        "    for line in lines:\n",
        "        line_counts[line] = line_counts.get(line, 0) + 1\n",
        "    lines = [line for line in lines if line_counts[line] < 5]\n",
        "    text = \" \".join(lines)\n",
        "\n",
        "    # NLP preprocessing: stopword removal + lemmatization\n",
        "    doc = nlp(text)\n",
        "    cleaned = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct and token.is_alpha]\n",
        "    return \" \".join(cleaned)\n"
      ],
      "metadata": {
        "id": "47AQ23bhc6pe"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_text=preprocess_text(text)"
      ],
      "metadata": {
        "id": "TeTBf5KBc6sy"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x3iK1uimc6vs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "T60PmgK-c6zJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_text_into_chunks(text, chunk_size=1700, chunk_overlap=50):\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap\n",
        "    )\n",
        "    return splitter.split_text(text)"
      ],
      "metadata": {
        "id": "jEn_IdP-dNHg"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_text_into_chunks(final_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8Q_eCqXpdNKT",
        "outputId": "997003a6-bb29-43bd-bba2-b16bde00b549"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['piece foreign order use skin skill high business item center service wide certain sound reach current office computer evidence establish fact society sing son election history world writer determine new share population perform course trial single effort find later strategy contain shake game car campaign likely require lot management partner kitchen eat agency type smile issue animal necessary identify guess method wife place partner response administration yeah save adult represent board change product admit inside maybe true standard rule author accept position concern situation buy husband manage vote address good wish hand find dark scientist memory simple research large exactly eat provide message east think audience law address indicate later wide environmental remember suggest song manage find speech conference appear finally year simple want buy lead thank action figure free card laugh send model agreement pass change simple catch stock huge actually cup relationship deal lose season voice book middle adult everybody meet friend process rise camera resource medium matter learn hit field simply technology lose nation institution wife teach weight term wrong growth reduce trouble news bill environment executive particular peace set cultural statement boy debate free pick sound ready art situation popular level likely care southern street defense development speak today expect need meet trip start control civil specific Sell leave big cold garden common think national relate laugh company dream friend meet tough program local find natural usually today possible source benefit system note read new model trade concern hour choice change avoid speech raise husband hour',\n",
              " 'choice change avoid speech raise husband hour score middle voice nature method determine enter clear education town assume southern dark participant total Miss scene generation want perform shake civil process executive manager southern hospital firm hear compare necessary international pretty pressure compare politic spring industry value able political ask run school tend instead truth agree look phone writer culture assume action mean soon sign production situation bear network federal station cup peace shake remain oil church bank seven job seat region recently list pretty speech mean base choice green age goal clearly tax billion drop particular student approach artist senior physical suddenly evening green describe road business Republican long sign turn miss population concern treat International probably decide exist pm work training effort sing water investment create financial traditional entire total debate Wall artist relationship star pressure decide exist general single probably deal challenge condition design worker scientist forget performance bank modern sort property positive quality employee head skin production rise tax compare eye general grow train wrong lot particularly present color writer relationship occur pull account Address pick significant Center include offer material drive build occur describe train management artist place information old fight red probably hit throw piece office company exist Star probably address president writer city bill travel age realize goal movement answer course executive cause executive task difference home war face today hope talk include smile response finish husband picture claim phone inside help avoid artist',\n",
              " 'picture claim phone inside help avoid artist turn fund risk animal generation executive security citizen available choose discussion major mouth energy note maybe early hit actually true seat sign perform director main discover animal home generation student Democrat challenge expect rest situation establish race piece activity investment difficult establish year plan guess guess large hospital sport important cover official fly fish environmental church church world realize religious control son feel student human garden pull center social purpose success join fire price ground safe produce technology expert brother add hand action oil pay note discussion field paper modern industry remember place kind state computer unit strong special chance central likely today wind control quickly majority traditional pull break shoulder man newspaper foreign white citizen nation idea land treatment case finish idea budget blood break address field close Doctor company city involve bill build food accord southern wish Congress civil particular especially decision detail attention reason star significant personal story crime little main decade send health traditional tough evening right reach strategy organization stay pm bring activity rich court bad common hold drug improve trade education decision thank hospital Mrs morning bill area idea guy score successful answer time collection find travel action computer stuff collection painting defense fire traditional important air challenge develop word small meeting fine memory woman appear population career doctor bar natural growth friend wish response available laugh appear Huge series Newspaper city black value choice personal event',\n",
              " 'Newspaper city black value choice personal event arm base tough form detail letter staff item country group Pay military boy coach value drug trial real traditional low interesting deep music position house hot paper look local police cause talk politic Democrat main establish reduce run sing federal House hot develop citizen face total health admit year box election cost Admit house appear laugh exist source feel challenge check Hour board sit marriage story field hour people hit line tend heart shake book success step start finally change cup stand society true Age friend war leave trip store miss lawyer lot medium view simply set rest current stop majority think agency large natural ahead radio free establish central director visit security particular purpose actually industry ready long protect news address project defense field understand western stuff force behavior despite school professional recognize specific occur describe way hour war traditional role time dog daughter talk street consumer guy economy add couple middle ago deep visit fast agreement push hard war pass standard agree leg light claim film information poor economy prepare agree boy question probably phone manager cold expert medical despite purpose court ahead agreement southern like sure improve visit head prove executive budget sell decide international second late drop develop despite Hair box source size hear mention action green risk agency operation impact standard word skin Century argue open couple social cut tell receive trouble power rule relationship issue war final address letter beautiful positive crime structure health modern talk tax board low positive culture community financial',\n",
              " 'board low positive culture community financial trial pay likely individual response catch size include claim staff avoid tend nearly court good study protect generation far staff measure study green family response address arrive deep tough election hope pattern heart include memory fire range director exactly mean measure magazine right education form forward consumer deep vote offer decision friend deep manage spring sister scientist Argue develop actually conference result pattern decide partner owner employee early open matter agree start candidate section catch gas evening positive career datum economy television money draw democratic social society record final concern believe special list century agree lawyer price behavior officer idea send base heart choice wrong well value fact tax tree important imagine direction story social character house push different start poor popular option education war Hair beautiful pick western market risk paint ready message ahead Bank maybe success develop week low eye management despite art fight material mother answer soldier season big rich form politic action future attention benefit officer impact win hand peace admit treatment stuff window spend beat information carry easy boy end try doctor color boy walk person leg wind wind power base eye campaign security role exactly hope second box citizen quality recently avoid hope sport system follow federal sure sure travel shoulder office kid Democrat effort open series owner daughter assume Professor enter nature citizen research senior song choice final author son practice commercial stand activity require citizen include action defense sister Mr true word soon cold control',\n",
              " 'defense sister Mr true word soon cold control probably bring nice think investment nearly open defense camera like ready begin popular assume medical pattern PM deep knowledge accept successful summer product news pressure rich care yeah person gun art rise fight station fast government church education member season age concern task similar lead need chair class son pass direction remain interview deep growth government learn way candidate argue religious finally care let analysis authority technology evening sit rest bed share late pm relate fish imagine reality ago base bag join window seven international coach network suffer billion rich close matter court maintain dark pm college tax try general song trouble class contain knowledge sister voice save cultural recent town science near marriage industry physical maintain certainly source hear quality success social soldier baby accept record trouble live commercial stop live hold character condition hot doctor focus history simply hotel week ahead glass old east begin stage Worker choice fast raise baby subject treat stand bear realize west listen economic watch thing compare serve character meeting discussion voice cover reality Shoulder sort year character line fall Goal discover commercial case edge kind friend subject feel collection kitchen effort week subject outside let action chair eye write business laugh control ground picture speech lawyer certain attack happen face american rise lie field evidence institution knowledge box carry theory remember decade man camera new president view speak model single area field arrive field leave thank property leg reveal inside feel exist right value arm benefit fine',\n",
              " 'inside feel exist right value arm benefit fine example camera skin right thank country matter attention area kitchen apply newspaper debate order participant father sister soldier young area think political experience relate power majority eat stuff add response career clear require reveal minute heart citizen break action provide join support choose occur Sea agree civil learn budget middle serve attack discuss policy Green memory project rise turn weight account leave ago special buy small result field evening product national tree year later newspaper arm result rock weight wear present specific moment hour actually attention middle professional high feel manage second month special push southern industry accord box effort history particularly sister life step security appear southern couple bad hour grow pick fund sound avoid institution firm natural avoid remember poor play pattern audience degree risk near heavy stuff feel success kid push reason board baby time management heart republican mind important government National card community shake pretty network speak play carry cultural yeah nice political policy talk remember laugh Use require mission interest majority modern war success base wonder charge benefit decision crime look report program leader represent animal sister fight describe way expert write east forget health man space concern executive ago half turn bring table far role bad argue quickly attention fire outside fear consider fast money note nation resource series seat turn defense despite discuss stage fight build total development vote appear ago fund early bar attack sport tough day draw compare public growth free note sound member subject',\n",
              " 'public growth free note sound member subject Worker Mention west knowledge watch away leave democratic bear owner current current glass business religious write agree reduce place number country option commercial difficult challenge attention individual contain remain economic movie throw couple Congress right approach fact deep leave knowledge land model official watch production especially information month painting interview visit specific bring concern add allow write design mother career smile finish word people benefit middle fish break service white nature significant reach tonight end pressure look happen determine customer sell clearly send tree new product understand federal page happen west maintain difference reveal rock bag moment character bag protect program blue public short bit difference south realize character key station walk size leg enter tend big increase meeting medical film position foot Knowledge site defense eye nature character mean need similar feel school article technology human wrong enjoy discussion ago catch environmental fast green detail south adult perform threat occur test customer job thing maintain far threat school theory remain manage kid firm west perform actually price push occur marriage couple watch claim today card successful table late north maybe develop budget bed development financial medium prove bill nearly site Bag budget truth impact create impact leave bar live finish address condition success day listen watch forget assume instead pattern different support eye phone bad want figure chair similar tv black possible far important fly increase door trip kind weight Place tell discussion worker mean cultural speak',\n",
              " 'Place tell discussion worker mean cultural speak record key candidate represent effort market support simply energy reach send maintain create probably train event employee inside Method think character kid program employee place democratic accord shoulder brother light interview pay agree shake admit computer door read american evidence season continue military political new letter billion machine shoulder like stand wrong ago guess trouble practice present tax hair hospital class fund idea movie voice high medium prove experience news allow happy paper like sister feel lay weight response exactly high public value major future player provide situation fine cause knowledge wrong capital player southern benefit purpose machine cell standard chance happy respond ready Wall pay yes author fast fast picture career notice college sense marriage director wife recently later hair choose pull begin pick president company nature ball catch occur quality actually long create statement discuss southern attorney visit Manager usually worry accept defense live prevent truth later form baby step policy center arm serve happen assume benefit send study argue hospital green major administration station language position air center need administration suddenly opportunity politics trial major green suffer society stay simple manage opportunity natural miss situation report big nature thank necessary Morning decide win term power art wall south board reflect nation blood feel natural window crime difficult work remember edge station politic organization enter job republican safe effort somebody fight growth record stuff personal option represent present money play play war challenge',\n",
              " 'represent present money play play war challenge store live stock far sell seat nation smile effect order stay home ago thank woman century middle mission security drug fine sell prevent set author seven meeting air treat rich rich range man important realize force accept pressure sport bag practice truth send significant agreement Bill especially tv nature sell shake drive opportunity note church help know quality world form account eat kind manager people food follow choice right night new question Mr center true care skin hope final campaign end change institution similar well analysis history push weight major risk morning usually recent thousand bad executive class shake peace relate story conference effort use day president fact produce large ability camera particularly main grow choose old head ask house doctor land Congress yeah hour wide establish program eye store tv wait role art head school brother start office leader usually paper account chair skin area price morning success style son investment response travel let particular century speech prepare establish mean increase plant time cold sign quickly wear item direction radio child raise note resource painting step year']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PROMPT_TEMPLATE = \"\"\"\n",
        "You are a document analysis assistant designed to extract structured metadata and generate summaries from text.\n",
        "\n",
        "Your task is to read the content below and return the following information in a strict JSON format:\n",
        "1. **title**: The title of the document (or an inferred title based on its content).\n",
        "2. **author**: The author’s name, if available, or return \"Unknown\".\n",
        "3. **date**: Date of publication or creation, if mentioned. Use \"Unknown\" if not found.\n",
        "4. **keywords**: A list of 4–7 relevant keywords or key phrases based on the document's topics.\n",
        "5. **document_type**: Choose the most appropriate type from:\n",
        "   - \"research paper\", \"legal notice\", \"resume\", \"report\", \"book chapter\",\n",
        "     \"article\", \"business proposal\", \"letter\", or \"others\".\n",
        "6. **summary**: A concise, neutral summary (3–5 sentences) covering the main points.\n",
        "\n",
        " Return your response strictly in this JSON format (no explanation, no markdown):\n",
        "\n",
        "{{\n",
        "  \"title\": \"\",\n",
        "  \"author\": \"\",\n",
        "  \"date\": \"\",\n",
        "  \"keywords\": [],\n",
        "  \"document_type\": \"\",\n",
        "  \"summary\": \"\"\n",
        "}}\n",
        "\n",
        "Content to analyze:\n",
        "\\\"\\\"\\\"{content_chunk}\\\"\\\"\\\"\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "XIa5DINRdNMw"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. llm_call.py\n",
        "import os\n",
        "import requests"
      ],
      "metadata": {
        "id": "zwNmnz6ddNPm"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"MISTRAL_API_URL\"] = \"https://api.mistral.ai/v1/chat/completions\"\n",
        "os.environ[\"MISTRAL_API_KEY\"] = \"xLy5WZAJHVd0AkGgkAcOO6X1psZWo0jY \"\n"
      ],
      "metadata": {
        "id": "wjFYCNfrdUcv"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MISTRAL_API_URL = os.getenv(\"MISTRAL_API_URL\")\n",
        "MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\")\n",
        "\n",
        "def call_llm_on_chunk(chunk):\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {os.getenv('MISTRAL_API_KEY')}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    data = {\n",
        "        \"model\": \"open-mistral-7b\",\n",
        "        \"messages\": [\n",
        "            {\"role\": \"user\", \"content\": PROMPT_TEMPLATE.format(content_chunk=chunk)}\n",
        "        ],\n",
        "        \"temperature\": 0.3\n",
        "    }\n",
        "    response = requests.post(os.getenv(\"MISTRAL_API_URL\"), headers=headers, json=data)\n",
        "    if response.status_code != 200:\n",
        "        print(f\"❌ Error {response.status_code}: {response.text}\")\n",
        "        return \"ERROR\"\n",
        "    return response.json()['choices'][0]['message']['content']\n"
      ],
      "metadata": {
        "id": "Ih7RscmFdUfU"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_document_chunks(chunks):\n",
        "    results = []\n",
        "    for chunk in chunks:\n",
        "        result = call_llm_on_chunk(chunk)\n",
        "        results.append(result)\n",
        "    return results"
      ],
      "metadata": {
        "id": "adipayApdUiM"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = extract_text_from_file('/content/generated_sample_3000_words.pdf')\n",
        "clean_text = preprocess_text(text)\n",
        "chunks = split_text_into_chunks(clean_text)"
      ],
      "metadata": {
        "id": "b3fiHRwCdUk_"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keybert sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Bix_utAidUoe",
        "outputId": "1ff56fd2-43dd-4ba4-f6d4-bbd56533bc4a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keybert\n",
            "  Downloading keybert-0.9.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from keybert) (2.0.2)\n",
            "Requirement already satisfied: rich>=10.4.0 in /usr/local/lib/python3.11/dist-packages (from keybert) (13.9.4)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.11/dist-packages (from keybert) (1.6.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.52.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.33.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.4.0->keybert) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.4.0->keybert) (2.19.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.2->keybert) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.2->keybert) (3.6.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.4.0->keybert) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.6.15)\n",
            "Downloading keybert-0.9.0-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.4/41.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, keybert\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed keybert-0.9.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keybert import KeyBERT\n",
        "from sentence_transformers import SentenceTransformer"
      ],
      "metadata": {
        "id": "Xy9qo3TAdNSc"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"✅ Total Chunks: {len(chunks)}\")\n",
        "\n",
        "results = []\n",
        "for i, chunk in enumerate(chunks):\n",
        "    print(f\"\\n--- Generating summary for Chunk {i+1}/{len(chunks)} ---\")\n",
        "    summary = call_llm_on_chunk(chunk)\n",
        "    print(summary)\n",
        "    results.append(summary)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pz0zJwbydeVW",
        "outputId": "0203bd86-9434-4a65-9951-56441f04a8fd"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Total Chunks: 10\n",
            "\n",
            "--- Generating summary for Chunk 1/10 ---\n",
            "{\n",
            "  \"title\": \"Unspecified Document\",\n",
            "  \"author\": \"Unknown\",\n",
            "  \"date\": \"Unknown\",\n",
            "  \"keywords\": [\"business\", \"skin\", \"service\", \"election\", \"research\", \"speech\", \"environment\"],\n",
            "  \"document_type\": \"others\",\n",
            "  \"summary\": \"The document discusses various topics including business, skin, service, election, research, and environmental issues. It also mentions speeches, elections, and a possible debate. The text appears to be related to a wide range of subjects, including politics, technology, and possibly aesthetics.\"\n",
            "}\n",
            "\n",
            "--- Generating summary for Chunk 2/10 ---\n",
            "{\n",
            "  \"title\": \"Unspecified Document\",\n",
            "  \"author\": \"Unknown\",\n",
            "  \"date\": \"Unknown\",\n",
            "  \"keywords\": [\"speech\", \"change\", \"education\", \"culture\", \"politics\", \"business\", \"international\"],\n",
            "  \"document_type\": \"others\",\n",
            "  \"summary\": \"The document discusses various topics including speech, change, education, culture, politics, business, and international relations. It seems to involve a debate or discussion, possibly about a speech or a proposal, and mentions the participation of several individuals, including an executive manager and a writer. The document also touches upon the importance of clear communication and understanding, as well as the role of modern technology in various aspects of life.\"\n",
            "}\n",
            "\n",
            "--- Generating summary for Chunk 3/10 ---\n",
            "{\n",
            "  \"title\": \"Unnamed Document on Various Topics\",\n",
            "  \"author\": \"Unknown\",\n",
            "  \"date\": \"Unknown\",\n",
            "  \"keywords\": [\"discussion\", \"decision\", \"politics\", \"education\", \"health\", \"technology\", \"art\"],\n",
            "  \"document_type\": \"others\",\n",
            "  \"summary\": \"The text discusses a variety of topics including decisions, politics, education, health, technology, and art. There are mentions of a discussion, a personal event, and a black newspaper city. The document also touches upon topics such as a doctor, a hospital, and a meeting.\"\n",
            "}\n",
            "\n",
            "--- Generating summary for Chunk 4/10 ---\n",
            "{\n",
            "  \"title\": \"Newspaper Article on Local Politics and Community Issues\",\n",
            "  \"author\": \"Unknown\",\n",
            "  \"date\": \"Unknown\",\n",
            "  \"keywords\": [\"local politics\", \"community issues\", \"election\", \"health\", \"crime\", \"music\", \"economy\"],\n",
            "  \"document_type\": \"article\",\n",
            "  \"summary\": \"This article discusses local politics, community issues, and the upcoming election. Topics include health, crime, music, economy, and the role of various groups such as the military and police. The article also touches upon the state of the community's financial situation and the need for improvement.\"\n",
            "}\n",
            "\n",
            "--- Generating summary for Chunk 5/10 ---\n",
            "{\n",
            "  \"title\": \"Unspecified Community Board Meeting Discussion\",\n",
            "  \"author\": \"Unknown\",\n",
            "  \"date\": \"Unknown\",\n",
            "  \"keywords\": [\"community\", \"financial trial\", \"culture\", \"election\", \"education\", \"politics\", \"debate\"],\n",
            "  \"document_type\": \"others\",\n",
            "  \"summary\": \"The text appears to be a discussion or minutes from a community board meeting, discussing various topics such as financial trials, culture, elections, education, and politics. The meeting also seems to address issues like community development, defense, and social matters. No specific details are provided about the nature of the discussions or decisions made.\"\n",
            "}\n",
            "\n",
            "--- Generating summary for Chunk 6/10 ---\n",
            "{\n",
            "  \"title\": \"Unavailable\",\n",
            "  \"author\": \"Unknown\",\n",
            "  \"date\": \"Unknown\",\n",
            "  \"keywords\": [\"defense\", \"camera\", \"control\", \"investment\", \"medical\", \"technology\", \"discussion\"],\n",
            "  \"document_type\": \"others\",\n",
            "  \"summary\": \"The text appears to discuss a potential situation involving defense, control, and technology, possibly in a medical or educational context. It also mentions investment, discussion, and a role for cameras. The document's type is not clearly defined, but it seems to be more of a conversation or a fragment of a larger piece.\"\n",
            "}\n",
            "\n",
            "--- Generating summary for Chunk 7/10 ---\n",
            "{\n",
            "  \"title\": \"Unspecified Topic\",\n",
            "  \"author\": \"Unknown\",\n",
            "  \"date\": \"Unknown\",\n",
            "  \"keywords\": [\"political\", \"policy\", \"debate\", \"community\", \"security\", \"development\", \"modern war\"],\n",
            "  \"document_type\": \"others\",\n",
            "  \"summary\": \"The text discusses a variety of topics including politics, policy, community, security, and modern war. It seems to involve a debate or discussion, possibly about development and security in a national context. The text also mentions a potential focus on money, resources, and power. The exact nature of the document is unclear.\"\n",
            "}\n",
            "\n",
            "--- Generating summary for Chunk 8/10 ---\n",
            "{\n",
            "  \"title\": \"Public Growth and Economic Challenges in the West\",\n",
            "  \"author\": \"Unknown\",\n",
            "  \"date\": \"Unknown\",\n",
            "  \"keywords\": [\"Public Growth\", \"Economic Challenges\", \"West\", \"Democracy\", \"Business\", \"Politics\", \"Cultural Differences\"],\n",
            "  \"document_type\": \"Article\",\n",
            "  \"summary\": \"This text appears to discuss public growth and economic challenges in the western region, focusing on political and cultural differences, business issues, and the impact on individuals and society. It mentions the need for a successful approach to manage these issues and maintain a stable economy.\"\n",
            "}\n",
            "\n",
            "--- Generating summary for Chunk 9/10 ---\n",
            "{\n",
            "  \"title\": \"Discussion on Worker Representation and Cultural Speak in the Modern Workplace\",\n",
            "  \"author\": \"Unknown\",\n",
            "  \"date\": \"Unknown\",\n",
            "  \"keywords\": [\"cultural speak\", \"worker representation\", \"modern workplace\", \"market support\", \"energy\", \"politics\", \"training\", \"employee\", \"democratic accord\", \"interview\"],\n",
            "  \"document_type\": \"article\",\n",
            "  \"summary\": \"The text discusses the importance of cultural speak in the modern workplace and the need for worker representation. It mentions the support of the market and the energy required to maintain and create a democratic environment. The document also touches upon the training of employees and the challenges they face, as well as the role of politics in the workplace.\"\n",
            "}\n",
            "\n",
            "--- Generating summary for Chunk 10/10 ---\n",
            "{\n",
            "  \"title\": \"Unspecified Topic\",\n",
            "  \"author\": \"Unknown\",\n",
            "  \"date\": \"Unknown\",\n",
            "  \"keywords\": [\"money\", \"war\", \"challenge\", \"live stock\", \"sell\", \"nation\", \"smile\", \"order\", \"home\", \"conference\"],\n",
            "  \"document_type\": \"others\",\n",
            "  \"summary\": \"The text discusses a variety of topics, including money, war, live stock, selling, and a conference, but lacks a clear structure and coherent narrative to provide a comprehensive summary.\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "combined_summaries = \"\\n\\n\".join(results)\n",
        "combine_prompt = f\"\"\"\n",
        "You are a smart metadata assistant. Below are partial summaries of a document generated from different chunks.\n",
        "\n",
        "Your task is to first read the chunks carefully to combine them into a **single, coherent metadata JSON object** with meaningful values.\n",
        "\n",
        "Infer the **title** and **author** based on the document as a whole, even if not explicitly mentioned.\n",
        "Generate a meaningful, concise **summary** for the full document.\n",
        "Merge and deduplicate the keywords intelligently.\n",
        "Assume the document type is \"Article\" unless there's a clear reason to choose otherwise.\n",
        "\n",
        "Return the result in this JSON format:\n",
        "{{\n",
        "  \"title\": \"Meaningful title of the whole document\",\n",
        "  \"author\": \"Author name (or 'Not specified' if not found)\",\n",
        "  \"date\": \"Not specified\",\n",
        "  \"keywords\": [\"keyword1\", \"keyword2\", \"...\"],\n",
        "  \"document_type\": \"Article\",\n",
        "  \"summary\": \"Clean, concise summary of the full document.\"\n",
        "}}\n",
        "\n",
        "Here are the partial summaries:\n",
        "\\\"\\\"\\\"{combined_summaries}\\\"\\\"\\\"\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "3nDtIIg_deYQ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import textwrap\n",
        "import re\n",
        "from keybert import KeyBERT\n",
        "from sentence_transformers import SentenceTransformer"
      ],
      "metadata": {
        "id": "UiaJIBxcdebL"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def call_llm_merge_summary(prompt):\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {os.getenv('MISTRAL_API_KEY')}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    data = {\n",
        "        \"model\": \"open-mistral-7b\",\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "        \"temperature\": 0.3\n",
        "    }\n",
        "    response = requests.post(os.getenv(\"MISTRAL_API_URL\"), headers=headers, json=data)\n",
        "    return response.json()['choices'][0]['message']['content']\n",
        "\n",
        "final_output = call_llm_merge_summary(combine_prompt)"
      ],
      "metadata": {
        "id": "r2d8CkGGdeeL"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "import textwrap\n",
        "from keybert import KeyBERT\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Example input: Replace this with your actual final_output and clean_text\n",
        "\n",
        "\n",
        "# ============================\n",
        "# 🎯 Final Output Handling\n",
        "# ============================\n",
        "try:\n",
        "    # ① Try fenced ```json ``` block\n",
        "    json_match = re.search(r'```json\\s+(.*?)\\s+```', final_output, re.DOTALL)\n",
        "\n",
        "    # ② If no fenced block found, try any JSON object in text\n",
        "    if not json_match:\n",
        "        json_match = re.search(r'\\{.*\\}', final_output, re.DOTALL)\n",
        "\n",
        "    # ③ If JSON found, parse it\n",
        "    if json_match:\n",
        "        json_string = json_match.group(0)\n",
        "        parsed = json.loads(json_string)\n",
        "\n",
        "        # ✅ Improve keywords using KeyBERT\n",
        "        if 'clean_text' in locals():\n",
        "            kw_model = KeyBERT(model=SentenceTransformer('all-MiniLM-L6-v2'))\n",
        "            kb_keywords = kw_model.extract_keywords(\n",
        "                clean_text,\n",
        "                keyphrase_ngram_range=(1, 2),\n",
        "                stop_words='english',\n",
        "                top_n=10,\n",
        "                use_maxsum=True,\n",
        "                nr_candidates=20\n",
        "            )\n",
        "            final_keywords = []\n",
        "            seen = set()\n",
        "            for kw, _ in kb_keywords:\n",
        "                if kw not in seen:\n",
        "                    seen.add(kw)\n",
        "                    final_keywords.append(kw)\n",
        "            parsed[\"keywords\"] = final_keywords\n",
        "        else:\n",
        "            print(\"⚠️ Warning: 'clean_text' not available for keyword extraction using KeyBERT.\")\n",
        "\n",
        "        # ✅ Pretty output\n",
        "        print(\"\\n✅ Final Metadata:\")\n",
        "        print(json.dumps(parsed, indent=2))\n",
        "\n",
        "        print(\"\\n✅ Final Summary:\")\n",
        "        if \"summary\" in parsed and parsed[\"summary\"]:\n",
        "            print(textwrap.fill(parsed[\"summary\"], width=100))\n",
        "        else:\n",
        "            print(\"Summary not available in the parsed output.\")\n",
        "\n",
        "    else:\n",
        "        print(\"⚠️ Could not find the JSON object within the final output string.\")\n",
        "        print(\"Showing raw output:\")\n",
        "        print(final_output)\n",
        "\n",
        "except json.JSONDecodeError as e:\n",
        "    print(f\"⚠️ JSONDecodeError: {e}\")\n",
        "    print(\"Showing raw output:\")\n",
        "    print(final_output)\n",
        "except KeyError as e:\n",
        "    print(f\"⚠️ KeyError: {e} - Check if expected keys are present in the JSON output.\")\n",
        "    if 'parsed' in locals():\n",
        "        print(\"Parsed dictionary (partial):\")\n",
        "        print(json.dumps(parsed, indent=2))\n",
        "    else:\n",
        "        print(\"Parsed dictionary not available.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pn80oBrgdesU",
        "outputId": "5829febc-1cd5-46fd-d90e-36bf6544fcfc"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n",
            "Showing raw output:\n",
            "Here is the combined metadata JSON object for the given document chunks:\n",
            "\n",
            "```json\n",
            "{\n",
            "  \"title\": \"Discussion on Various Topics in Modern Society\",\n",
            "  \"author\": \"Not specified\",\n",
            "  \"date\": \"Not specified\",\n",
            "  \"keywords\": [\"business\", \"skin\", \"service\", \"election\", \"research\", \"speech\", \"environment\", \"change\", \"education\", \"culture\", \"politics\", \"international\", \"discussion\", \"decision\", \"local politics\", \"community issues\", \"financial trial\", \"health\", \"crime\", \"music\", \"economy\", \"defense\", \"camera\", \"control\", \"investment\", \"medical\", \"technology\", \"policy\", \"debate\", \"community\", \"security\", \"development\", \"modern war\", \"Public Growth\", \"Economic Challenges\", \"West\", \"Democracy\", \"worker representation\", \"cultural speak\", \"market support\", \"energy\", \"training\", \"employee\", \"democratic accord\", \"interview\"],\n",
            "  \"document_type\": \"Article\",\n",
            "  \"summary\": \"The combined document discusses a wide range of topics in modern society, including business, politics, technology, and cultural issues. It touches upon local politics and community issues, as well as global topics like international relations, defense, and economic challenges in the West. The text also discusses the importance of worker representation and cultural speak in the modern workplace, and the role of politics in various aspects of life.\"\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install streamlit pyngrok --quiet\n"
      ],
      "metadata": {
        "id": "hOeKqAVJgm_8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dd86366-7596-4c66-dafb-8e95c615a98b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install streamlit -q\n"
      ],
      "metadata": {
        "id": "AzJnWYAMy0mw"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "# app.py\n",
        "import os\n",
        "import json\n",
        "import textwrap\n",
        "import tempfile\n",
        "from pathlib import Path\n",
        "from typing import List\n",
        "\n",
        "import streamlit as st\n",
        "import requests\n",
        "from keybert import KeyBERT\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from docx import Document\n",
        "import pdfplumber\n",
        "\n",
        "# Load KeyBERT model once\n",
        "kw_model = KeyBERT()\n",
        "\n",
        "# Prompt used per chunk\n",
        "PROMPT_TEMPLATE = \"\"\"\n",
        "You are an intelligent assistant. Read this content chunk and return:\n",
        "- A 1-2 sentence summary\n",
        "- Five keywords (comma-separated)\n",
        "\n",
        "Chunk:\n",
        "\\\"\\\"\\\"{content_chunk}\\\"\\\"\\\"\n",
        "\"\"\"\n",
        "\n",
        "# ───────────────────────────────\n",
        "# Utility Functions\n",
        "# ───────────────────────────────\n",
        "\n",
        "def extract_text_from_file(uploaded_file) -> str:\n",
        "    suffix = Path(uploaded_file.name).suffix.lower()\n",
        "    with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp:\n",
        "        tmp.write(uploaded_file.read())\n",
        "        tmp_path = Path(tmp.name)\n",
        "\n",
        "    if suffix == \".txt\":\n",
        "        return tmp_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "    elif suffix == \".docx\":\n",
        "        doc = Document(tmp_path)\n",
        "        return \"\\n\".join([p.text for p in doc.paragraphs])\n",
        "    elif suffix == \".pdf\":\n",
        "        text = \"\"\n",
        "        with pdfplumber.open(tmp_path) as pdf:\n",
        "            for page in pdf.pages:\n",
        "                text += page.extract_text() or \"\"\n",
        "        return text\n",
        "    raise ValueError(f\"Unsupported file type: {suffix}\")\n",
        "\n",
        "def preprocess_text(text: str) -> str:\n",
        "    return \" \".join(text.split())\n",
        "\n",
        "def split_text_into_chunks(text: str, size: int = 1700, overlap: int = 50) -> List[str]:\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=size, chunk_overlap=overlap)\n",
        "    return splitter.split_text(text)\n",
        "\n",
        "def call_mistral(prompt: str, temperature: float = 0.3) -> str:\n",
        "    api_url = os.getenv(\"MISTRAL_API_URL\")\n",
        "    api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
        "    if not api_url or not api_key:\n",
        "        raise ValueError(\"Please set MISTRAL_API_URL and MISTRAL_API_KEY env vars\")\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {api_key}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    data = {\n",
        "        \"model\": \"open-mistral-7b\",\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "        \"temperature\": temperature\n",
        "    }\n",
        "    resp = requests.post(api_url, headers=headers, json=data, timeout=60)\n",
        "    resp.raise_for_status()\n",
        "    return resp.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "\n",
        "# ───────────────────────────────\n",
        "# Streamlit UI\n",
        "# ───────────────────────────────\n",
        "\n",
        "st.set_page_config(page_title=\"Metadata & Summary Generator\", layout=\"centered\")\n",
        "\n",
        "st.markdown(\n",
        "    '<h1 style=\"color:#4A90E2;text-align:center;\">📄 Auto Metadata & Summary Generator</h1>',\n",
        "    unsafe_allow_html=True\n",
        ")\n",
        "\n",
        "file = st.file_uploader(\"📂 Upload PDF, DOCX, or TXT file\", type=[\"pdf\", \"docx\", \"txt\"])\n",
        "\n",
        "if file:\n",
        "    with st.spinner(\"⏳ Processing the uploaded file...\"):\n",
        "        raw_text = extract_text_from_file(file)\n",
        "        clean_text = preprocess_text(raw_text)\n",
        "        chunks = split_text_into_chunks(clean_text)\n",
        "\n",
        "        summaries = [call_mistral(PROMPT_TEMPLATE.format(content_chunk=chunk)) for chunk in chunks]\n",
        "        combined = \"\\n\\n\".join(summaries)\n",
        "\n",
        "        # Final summarization prompt\n",
        "        final_prompt = f\"\"\"\n",
        "You are a smart metadata assistant. Below are partial summaries of a document generated from different chunks.\n",
        "\n",
        "Your task is to first read the chunks carefully to combine them into a **single, coherent metadata JSON object** with meaningful values.\n",
        "\n",
        "Infer the **title** and **author** based on the document as a whole, even if not explicitly mentioned.\n",
        "Generate a meaningful, concise **summary** for the full document.\n",
        "Merge and deduplicate the keywords intelligently.\n",
        "Assume the document type is \"Article\" unless there's a clear reason to choose otherwise.\n",
        "\n",
        "Return the result in this JSON format:\n",
        "{{\n",
        "  \"title\": \"Meaningful title of the whole document\",\n",
        "  \"author\": \"Author name (or 'Not specified' if not found)\",\n",
        "  \"date\": \"Not specified\",\n",
        "  \"keywords\": [\"keyword1\", \"keyword2\", \"...\"],\n",
        "  \"document_type\": \"Article\",\n",
        "  \"summary\": \"Clean, concise summary of the full document.\"\n",
        "}}\n",
        "\n",
        "Here are the partial summaries:\n",
        "\\\"\\\"\\\"{combined}\\\"\\\"\\\"\n",
        "\"\"\"\n",
        "        final_output = call_mistral(final_prompt)\n",
        "\n",
        "    try:\n",
        "        parsed = json.loads(final_output)\n",
        "\n",
        "        # Optional: Use KeyBERT for better keywords\n",
        "        kb_keywords = kw_model.extract_keywords(\n",
        "            clean_text,\n",
        "            keyphrase_ngram_range=(1, 2),\n",
        "            stop_words=\"english\",\n",
        "            top_n=10,\n",
        "            use_maxsum=True,\n",
        "            nr_candidates=20\n",
        "        )\n",
        "        parsed[\"keywords\"] = [kw for kw, _ in kb_keywords]\n",
        "\n",
        "        # ── Styled Output ──\n",
        "        st.markdown('<h3 style=\"color:#1f77b4;\">📌 <b>Extracted Metadata</b></h3>', unsafe_allow_html=True)\n",
        "        st.json(parsed)\n",
        "\n",
        "        st.markdown('<h3 style=\"color:#2ca02c;\">📝 <b>Wrapped Summary</b></h3>', unsafe_allow_html=True)\n",
        "        st.markdown(\n",
        "            f\"<div style='color:#333333; font-size:16px; line-height:1.6; background-color:#f4f4f4; padding:15px; border-radius:8px'>{parsed['summary']}</div>\",\n",
        "            unsafe_allow_html=True\n",
        "        )\n",
        "        st.markdown(\"<br>\", unsafe_allow_html=True)\n",
        "\n",
        "        st.download_button(\n",
        "            label=\"💾 Download Summary\",\n",
        "            data=parsed[\"summary\"],\n",
        "            file_name=\"summary.txt\",\n",
        "            mime=\"text/plain\"\n",
        "        )\n",
        "\n",
        "        st.markdown(\"<hr><div style='text-align:center;color:#888'>Made with ❤️ by Arpit · Powered by Mistral AI</div>\", unsafe_allow_html=True)\n",
        "\n",
        "    except Exception as e:\n",
        "        st.error(f\"❌ Failed to parse output: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2REES6yDy0pW",
        "outputId": "825892f6-e998-4b9a-f12b-c2a7ec8887dd"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# In Colab or local terminal:\n",
        "!streamlit run app.py & npx localtunnel --port 8501\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZG6Lfbh5vDi",
        "outputId": "80f3a3d2-23f4-4a1c-bd46-633a4f76c29a",
        "collapsed": true
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://104.199.121.54:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0Kyour url is: https://fifty-bees-watch.loca.lt\n",
            "2025-06-24 02:52:49.246803: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1750733569.312134    7962 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1750733569.332233    7962 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ypZfAqza5vF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vTyg8d7r5vIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HaDbDhc-5vMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zy-Lc40t3pRM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}